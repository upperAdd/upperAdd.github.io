#  Portfolio

### A sample of my personal and work projects.


---
### Machine Learning Unsupervised  -  Clustering Countries
[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/upperAdd/Machine-Learning-Unsupervised)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
<p align="justify">
  
</p>
<center><img src="images/png?raw=true"/></center>


---
### Time Series Analysis  -  Electricity Wholesale prediction price
[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/upperAdd/final-project)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
<p align="justify">For my final project, I choose Time Series analysis because I want to know what the average energy bill will be this winter. Therefore, I choose to use a Nordpool wholesale electricity price dataset from 2018 until the 6th of august 2022. According to Ofgem, wholesale prices are 40% of our energy bill. I split the dataset into 80/20 train and test data and used time series cross-validation for this project as we can’t use the grid search cross-validation. The reason for choosing this type is because time series data characteristic by the dependence of data observation are close in time. Using the standard K-fold cross-validation method would lead to an unreasonable correlation between training and testing instances, leading to poor estimation of generalizing error. The other engineering features I used are lag features, and I chose 3 times the fold due to the amount of train data used.</p>
<center><img src="images/electricity.png?raw=true"/></center>

---
### Machine Learning  -  Fake News
[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/upperAdd/Fake-News-)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
<p align="justify">In this digital era, we often can’t distinguish between hoax news and fact-finding news. Especially the spread of misinformation on social media platforms is an ever-growing problem. Organizations, politicians, individuals looking for personal gain and even certain news media outlets propagate fake news to sway people's decisions and distort events to fit a bias or prejudice. The authenticity of the news posted online cannot be definitively measured since the manual classification of news is tedious and time-consuming and is also subject to bias. Therefore, this assignment aims to use machine learning Supervised Random Forest regression and AdaBoost classifier.</p>
<center><img src="images/fake news.png?raw=true"/></center>


---
### Neural Network -  Churn Prediction
[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/upperAdd/Neural-Networks)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
<p align="justify">This project aims to predict the percentage of customer churn in the bank. Churn is a terminology that is commonly used in business companies. Churn rate is the rate at which customers stop doing business with a company over a given period of time. It can also describe customers stopping or cancelling subscriptions. For this project, we need to use Artificial Neural Network (ANN) or be familiar with Neural Network for machine learning modelling. After data preprocessing, we scale and split the dataset (70/30 test) on the data wrangling process. To check the result, I decided to use machine learning models: decision tree, logistic regression and support vector machines.</p>
<center><img src="images/ANN.png?raw=true"/></center>


---
### Logistic Regression -  Titanic
[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/upperAdd/Logistic-Regression-Assignment)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
<p align="justify">Titanic are the most popular case to study logistic regression. Likewise, the dataset for titanic we get from Kaggle. The aim of this assignment is to predict the survivor when it happens. For this logistic regression for feature engineering, we need to do cross-validation using GridSearchCV, and then after that, we split our data set by 70/30 train and test data. We also need to check the accuracy of our score. Besides logistic regression, we also use naïve Bayes to compare the value.</p>
<center><img src="images/logistic.png?raw=true"/></center>


---
### Linear Regression -  JFK Flight Delay 
[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/upperAdd/Linear-Regression-Assignment)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
<p align="justify">This project also used a dataset from Kaggle and the dataset from JFK Airport between Nov 2019 – Dec 2020. The aim of this assignment is to check what potential direct impact the cost of the flight. As we know, the taxiing process in airlines consumes 5-10% of total fuel consumption, and Total Fuel Consumption takes 30% of the Total Operation Cost of each aircraft. Therefore, we can conclude that efficiently taxiing can reduce the TOC. In this project, I am trying to see which airlines often delay departure from their scheduled departures and try to predict the delay coefficient from their actual departures.</p>
<center><img src="images/linear.png?raw=true"/></center>


---
### Data Visualisation -  Earthquake
[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/upperAdd/DV_assigment)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
<p align="justify">This project is my first assignment doing data pre-processing and data visualisation. We use the dataset from Kaggle. In data science, data pre-processing refers to the step from extracting a dataset from the source until we clean our dataset from any duplicated or missing values. After that, our dataset is ready to explore or what we used to call Exploratory Data Analysis. On my first EDA that showed on my pie chart, I checked the top five locations with the biggest magnitude and if those locations were under the “Ring of Fire” or just in the most seismic region. Astonishingly based on exploration data, those locations are not the most frequently intense tectonic activities. The top five locations where earthquakes often occur in sequences were in the same county in America, CA, with 3 locations in the same place, “The Geysers”.
</p>
<center><img src="images/dv-eq.png?raw=true"/></center>


---
### Python Game - Adventure Game
[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/upperAdd/Adventure-Game-DA_week-2)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
<p align="justify">This project is my favourite assignment to learn python as a programming language. We need to create adventure games that make the reader try to go into several rooms and have some blockage on the way out. I created this game inspired by Alice in Wonderland, but the situation I changed into vacation into a tropical island where readers need to have a diving license or advanced swimming skills to enjoy the vacation. I used a different colour to distinguish someone speaking either by themself or in a conversation between other people and the narrative story. I also put some intro to describe the situation on the beach before the reader starts the game to choose which path they want. Below is the screenshot of the game that I build.</p>
<img src="images/komodo island.png?raw=true"/>
<video src="images/adventure game.mp4" width=500px height=500px autoplay controls></video>
---

<left> © 2022 Titi P.</left>
